<!DOCTYPE html>

<html>
  <head>
    <title>Affine Website</title>
    <br id="index" hidden/>
    <meta charset="utf-8"/>
    <link href="https://fonts.googleapis.com/css?family=Roboto" rel="stylesheet">
    <link rel="stylesheet" type="text/css" href="style.css">
  </head>

  <body>
        <div id="container">
            <h3>Written Work</h3>
            <p><a href="mphil_thesis.html">MPhil Thesis</a> on approximate variational inference in Gaussian processes when there is circulant structure present in the pseudo-point covariance matrix. The corresponding <a href="mphil_poster.html">poster</a> was presented to the course's industrial sponsors.</p>
            <p>An <a href=aabi_2016_paper.html>extension</a> of the above MPhil thesis was accepted for <a href="aabi_2016_poster.html">poster presentation</a> at the NIPS workshop on Advances in Approximate Bayesian Inference (Barcelona, 2016).</p>

            <h3>Presentations</h3>
            <p><a href="resources/gaussian-affine-posterior.pdf">Conditioning in Gaussians is an Affine Transform</a>. A (very) short presentation on a cute result that I initially found surprising. This presentation is rather terse, but my current research agenda employs this result, so a more thorough explanation of this result will be available here in the near future. Many thanks to <a href="https://wesselb.github.io/">Wessel Bruinsma</a> for sanity checking this result.</p>
            <p><a href="resources/learning-to-learn.pdf">Learning to Learn</a>: a presentation written and presented jointly with <a href="http://mlg.eng.cam.ac.uk/?portfolio=siddharth-swaroop">Siddharth Swaroop</a>. Here we review a couple of themes in the nebulously-defined sub-field of ML known as "Learning to Learn" or "Meta-Learning". This presentation is not intended to provide extensive detail for any particular work, but rather to frame two research themes in the area of Learning to Learn in a concise manner.</p>

            <h3>(Public) Implementations</h3>
            <p><a href="https://www.github.com/invenia/Nabla.jl">Nabla.jl</a> is a Reverse-Mode automatic differentiation (RMAD) package written in Julia, of which I am the primary author. The majority of the initial implementation of this package was undertaken in the first half of 2017 whilst working for Invenia Labs (Cambridge, UK), but development is on-going. We have focused on optimising linear algebra (e.g. chol, (log)det, \, / etc) and higher-order functions (e.g. broadcast, map, mapreduce etc) as these have not yet received as much attention in the Julia community as would be ideal. There is an on-going effort in the Julia community to improve native Julia support and to converge upon a single native RMAD package; it is my hope that the distinct aspects of Nabla.jl will be able to be incorporated into this package.</p>
        </div>
    </body>
</html>
