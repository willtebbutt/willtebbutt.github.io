<!DOCTYPE html>

<html>
  <head>
    <title>Affine Website</title>
    <br id="index" hidden/>
    <meta charset="utf-8"/>
    <link href="https://fonts.googleapis.com/css?family=Roboto" rel="stylesheet">
    <link rel="stylesheet" type="text/css" href="style.css">
  </head>

  <body>
        <div id="container">

            <h3>About</h3>
            <p>I'm a PhD student in the <a href=http://mlg.eng.cam.ac.uk/>Machine Learning Group</a> at Cambridge, supervised by <a href=http://learning.eng.cam.ac.uk/Public/Turner/WebHome>Rich Turner</a>. I'm generally interested in probabilistic modelling and (approximate) inference, how Gaussian processes should feature in probabilistic programming, and how to scale GPs for large time series and spatio-temporal problems.</p>

            <p>The above feeds directly into my work on probabilistic machine learning in climate science, which addresses combining the predictions of ensembles of GCMs in a sensible way, and the requirements that this task places on statistical weather modelling.</p>


            <h3>Research Highlights</h3>

            <h4>Gaussian Process Probabilistic Programming</h4>

            <p>Gaussian process probabilistic programming (GPPP) is a term I've coined for the work I'm doing to re-design the way that we work with GPs in a practical sense. I presented the high-level aspects of this work at <a href="resources/gppp_probprog.pdf">ProbProg</a> and <a href="resources/gppp_juliacon.pdf">JuliaCon</a>, and continue to work on a paper.

            <p>Implementations available in <a href=https://github.com/willtebbutt/Stheno.jl>Julia</a> and <a href=https://github.com/wesselb/stheno>Python</a></p>

            <h4>GPAR</h4>
            <p><a href="https://arxiv.org/abs/1802.07182">GPAR</a> is a multi-output Gaussian process-based regression model. It's simple, scalable by GP standards, and seems to work really well in practice. In particular it overcomes some of the limitations of standard multi-output GPs. It appeared at AISTATS 2019.</p>

            <p>Implementations available in <a href="https://github.com/wesselb/gpar">Python</a> and <a href=https://github.com/willtebbutt/GPARs.jl/>Julia</a>.</p>

            <h4>The OILMM</h4>
            <p>The <a href="https://arxiv.org/pdf/1911.06287.pdf">OILMM</a> is another multi-output Gaussian process model for regression, that's easily able to handle a lot of outputs while retaining exact inference -- Wessel came up with the main idea and I contributed the link between it and separable spatio-temporal processes, and ideas for experiments. It appeared at ICML 2020.</p>

            <p>Implementations available in <a href="https://github.com/wesselb/oilmm">Python</a> and <a href="https://github.com/willtebbutt/OILMMs.jl">Julia</a>.</p>


            <h3>Software</h3>
            <h4><a href="https://github.com/willtebbutt/Stheno.jl">Stheno.jl</a></h4>

            <p>This is a Julia implementation of my GPPP work. It's permanently ongoing, but makes working with GPs in problems involving multiple related processes signficantly more straightforward than traditional GP packages.</p>

            <p>JuliaCon 2019: <a href="https://www.youtube.com/watch?v=OO3BBkGEMV8">talk</a> and <a href="resources/stheno_juliacon_2019.pdf">slides</a>.</p>

            <h4><a href="https://github.com/willtebbutt/TemporalGPs.jl/">TemporalGPs.jl</a></h4>

            <p>This work implements SDE approximations to GPs, which dramatically accelerates inference and learning for models involving long time horizons. This is notable because the standard pseudo-point approximations fail in these scenarios.</p>

            <p>JuliaCon 2020: <a href=https://www.youtube.com/watch?v=dysmEpX1QoE>talk</a> and <a href="resources/juliacon-2020.pdf">slides</a>.</p>
        </div>
    </body>
</html>
