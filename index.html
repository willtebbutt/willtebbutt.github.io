<!DOCTYPE html>

<html>
  <head>
    <title>Affine Website</title>
    <br id="index" hidden/>
    <meta charset="utf-8"/>
    <link href="https://fonts.googleapis.com/css?family=Roboto" rel="stylesheet">
    <link rel="stylesheet" type="text/css" href="style.css">
  </head>

  <body>
        <div id="container">

            <h3>About</h3>
            <p>I'm a PhD student in the <a href=http://mlg.eng.cam.ac.uk/>Machine Learning Group</a> at Cambridge, supervised by <a href=http://learning.eng.cam.ac.uk/Public/Turner/WebHome>Rich Turner</a>. I'm generally interested in probabilistic modelling and (approximate) inference, how Gaussian processes should feature in probabilistic programming, and how to scale GPs for large time series and spatio-temporal problems.</p>

            <p>The above feeds directly into my work on probabilistic machine learning in climate science, which addresses combining the predictions of ensembles of GCMs in a sensible way, and the requirements that this task places on statistical weather modelling.</p>


            <h3>Research Highlights</h3>

            <h4>Combining Pseudo-Point and State Space Approximations for Sum-Separable Gaussian Processes</h4>

            <p>
                <a href="https://arxiv.org/abs/2106.10210">This work</a> shows how to combine pseudo-point (a.k.a sparse, inducing point, etc) and state space
                approximations for GPs -- this is a good idea because they have
                complementary strengths and weaknesses, and combining them yields the benefits
                of both approximations.
                In this work we showed that combining them is remarkably simple, and provide
                straightforward-to-implement methods for approximate learning and inference.
                It's not a priori obvious how to do this in a principled way -- and our main
                contribution is showing that, due to a little-known existing result, the
                "right" way to do this is the "obvious" one.
                We specifically show how to do this using the widely-favoured variational
                pseudo-point approximation, but our result generalises to other flavours of
                pseudo-point approximation.
                In particular, our work shines a new light on older work that constructed a
                very similar approximation using FITC.
                Appeared at UAI 2021.
            </p>

            <p>
                Julia implementation of the technique available in <a href="https://github.com/willtebbutt/TemporalGPs.jl/">TemporalGPs.jl</a> (see examples).
                Code to reproduce experiments available <a href="https://github.com/willtebbutt/PseudoPointStateSpace-UAI-2021">here</a>.
            </p>

            <h4>Gaussian Process Probabilistic Programming</h4>

            <p>Gaussian process probabilistic programming (GPPP) is a term I've coined for the work I'm doing to re-design the way that we work with GPs in a practical sense. I presented the high-level aspects of this work at <a href="resources/gppp_probprog.pdf">ProbProg</a> and <a href="resources/gppp_juliacon.pdf">JuliaCon</a>, and continue to work on a paper.

            <p>Implementations available in <a href=https://github.com/willtebbutt/Stheno.jl>Julia</a> and <a href=https://github.com/wesselb/stheno>Python</a></p>

            <h4>GPAR</h4>
            <p><a href="https://arxiv.org/abs/1802.07182">GPAR</a> is a multi-output Gaussian process-based regression model. It's simple, scalable by GP standards, and seems to work really well in practice. In particular it overcomes some of the limitations of standard multi-output GPs. It appeared at AISTATS 2019.</p>

            <p>Implementations available in <a href="https://github.com/wesselb/gpar">Python</a> and <a href=https://github.com/willtebbutt/GPARs.jl/>Julia</a>.</p>

            <h4>The OILMM</h4>
            <p>The <a href="https://arxiv.org/pdf/1911.06287.pdf">OILMM</a> is another multi-output Gaussian process model for regression that's easily able to handle a lot of outputs while retaining exact inference. It's really Wessel's thing -- he came up with the main idea, and he and Eric did most of the work. I pointed out the connection with separable spatio-temporal processes and state space approximations, some ideas for experiments, and used <a href="https://github.com/willtebbutt/TemporalGPs.jl/">TemporalGPs.jl</a> to run one of the experiments. It appeared at ICML 2020.</p>

            <p>Implementations available in <a href="https://github.com/wesselb/oilmm">Python</a> and <a href="https://github.com/willtebbutt/OILMMs.jl">Julia</a>.</p>


            <h3>Software</h3>
            <h4><a href="https://github.com/willtebbutt/Stheno.jl">Stheno.jl</a></h4>

            <p>This is a Julia implementation of my GPPP work. It's permanently ongoing, but makes working with GPs in problems involving multiple related processes signficantly more straightforward than traditional GP packages.</p>

            <p>JuliaCon 2019: <a href="https://www.youtube.com/watch?v=OO3BBkGEMV8">talk</a> and <a href="resources/stheno_juliacon_2019.pdf">slides</a>.</p>

            <h4><a href="https://github.com/willtebbutt/TemporalGPs.jl/">TemporalGPs.jl</a></h4>

            <p>This work implements SDE approximations to GPs, which dramatically accelerates inference and learning for models involving long time horizons. This is notable because the standard pseudo-point approximations fail in these scenarios.</p>

            <p>JuliaCon 2020: <a href=https://www.youtube.com/watch?v=dysmEpX1QoE>talk</a> and <a href="resources/juliacon-2020.pdf">slides</a>.</p>
        </div>
    </body>
</html>
